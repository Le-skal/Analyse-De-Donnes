## **1. ENCODAGE DES VARIABLES CATÃ‰GORIELLES**

### **ProblÃ¨me de base**
Les modÃ¨les de ML ne comprennent que les chiffres. Donc on doit convertir les catÃ©gories (texte) en nombres.

### **One-Hot Encoding (variables nominales)**
**C'est quoi une variable nominale ?** Une catÃ©gorie sans ordre logique.

**Exemple avec `person_home_ownership`** :
- Valeurs possibles : RENT, OWN, MORTGAGE
- Pas d'ordre : on ne peut pas dire que OWN > RENT ou MORTGAGE > OWN

**Comment Ã§a marche ?**
On crÃ©e une colonne binaire (0 ou 1) pour chaque catÃ©gorie :

```
Avant :
| person_home_ownership |
|-----------------------|
| RENT                  |
| OWN                   |
| MORTGAGE              |

AprÃ¨s One-Hot Encoding :
| home_RENT | home_OWN | home_MORTGAGE |
|-----------|----------|---------------|
| 1         | 0        | 0             |
| 0         | 1        | 0             |
| 0         | 0        | 1             |
```

### **Ordinal Encoding (variables ordinales)**
**C'est quoi une variable ordinale ?** Une catÃ©gorie avec un ordre logique.

**Exemple avec `loan_grade`** :
- Valeurs possibles : A, B, C, D, E, F, G
- IL Y A un ordre : A est meilleur que B, B meilleur que C, etc.

**Comment Ã§a marche ?**
On remplace chaque catÃ©gorie par un nombre qui respecte l'ordre :

```
| loan_grade | â†’ | loan_grade_encoded |
|------------|---|-------------------|
| A          | â†’ | 0                 |
| B          | â†’ | 1                 |
| C          | â†’ | 2                 |
| D          | â†’ | 3                 |
| E          | â†’ | 4                 |
| F          | â†’ | 5                 |
| G          | â†’ | 6                 |
```

**Pourquoi pas One-Hot pour loan_grade ?** Parce qu'on perdrait l'information d'ordre. Le modÃ¨le ne saurait pas que A < B < C.

---

## **2. STRATIFIED SPLIT**

### **Le problÃ¨me**
Tu as un dataset dÃ©sÃ©quilibrÃ© :
- 78% de classe 0 (remboursÃ©)
- 22% de classe 1 (dÃ©faut)

### **Split normal (MAUVAIS)**
Si tu fais un split alÃ©atoire simple (80% train / 20% test), tu peux avoir un problÃ¨me :

```
Dataset complet : 78% classe 0, 22% classe 1

Train (80%) : pourrait avoir 82% classe 0, 18% classe 1  âŒ
Test (20%)  : pourrait avoir 65% classe 0, 35% classe 1  âŒ
```

Les proportions ne sont pas respectÃ©es ! Le modÃ¨le s'entraÃ®ne sur une distribution diffÃ©rente de celle du test.

### **Stratified Split (BON)**
Il **garantit** que les proportions sont identiques partout :

```
Dataset complet : 78% classe 0, 22% classe 1

Train (80%) : 78% classe 0, 22% classe 1  âœ…
Test (20%)  : 78% classe 0, 22% classe 1  âœ…
```

**Code Python** :
```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, 
    test_size=0.2, 
    stratify=y,  # â† Ã‡a c'est le stratified split
    random_state=42
)
```

**Pourquoi c'est important ?**
- Ã‰vite le biais d'Ã©chantillonnage
- Le modÃ¨le voit la mÃªme distribution en train et en test
- Les mÃ©triques de performance sont plus fiables

---

## **3. LES 3 MODÃˆLES DE MACHINE LEARNING (VERSION SIMPLE)**

### **ðŸ“Š RÃ‰GRESSION LOGISTIQUE**

**Analogie simple** : C'est comme tracer une ligne pour sÃ©parer les bons et les mauvais emprunteurs.

**Comment Ã§a marche ?**

1. **Ã‰tape 1 : Combinaison linÃ©aire**
   ```
   z = Î²â‚€ + Î²â‚Ã—(revenu) + Î²â‚‚Ã—(montant_prÃªt) + Î²â‚ƒÃ—(Ã¢ge) + ...
   ```
   - Î²â‚€, Î²â‚, Î²â‚‚... sont des poids appris par le modÃ¨le
   - z est un score (peut Ãªtre nÃ©gatif ou positif)

2. **Ã‰tape 2 : Fonction sigmoÃ¯de**
   ```
   ProbabilitÃ© de dÃ©faut = 1 / (1 + e^(-z))
   ```
   - Transforme z en probabilitÃ© entre 0 et 1
   - Si z est trÃ¨s nÃ©gatif â†’ probabilitÃ© proche de 0 (bon client)
   - Si z est trÃ¨s positif â†’ probabilitÃ© proche de 1 (mauvais client)

3. **DÃ©cision finale**
   ```
   Si probabilitÃ© > 0.5 â†’ PrÃ©diction : DÃ‰FAUT (classe 1)
   Si probabilitÃ© < 0.5 â†’ PrÃ©diction : REMBOURSÃ‰ (classe 0)
   ```

**ParamÃ¨tres principaux** :
- `C` : RÃ©gularisation (plus C est petit, plus le modÃ¨le est simple)
- `penalty` : Type de rÃ©gularisation (L1, L2)
- `class_weight` : Ajuste les poids des classes (important pour dÃ©sÃ©quilibre)

**Avantages** :
- Simple, rapide
- InterprÃ©table (on voit l'impact de chaque variable)

**Limites** :
- Suppose une relation linÃ©aire
- Peu performant si les relations sont complexes

---

### **ðŸŒ³ RANDOM FOREST**

**Analogie simple** : C'est comme demander l'avis de 100 experts, puis voter pour la dÃ©cision finale.

**Comment Ã§a marche ?**

1. **CrÃ©er plein d'arbres de dÃ©cision**
   - Chaque arbre pose des questions : "Le revenu est-il > 50kâ‚¬ ?", "Le grade est-il < C ?"
   - Chaque arbre est entraÃ®nÃ© sur un Ã©chantillon alÃ©atoire des donnÃ©es

2. **Exemple d'arbre** :
   ```
                   Revenu > 50k ?
                   /            \
                 OUI            NON
                  |              |
         Grade < C ?      DÃ©faut passÃ© ?
         /        \          /        \
       OUI       NON       OUI       NON
        |         |         |         |
     Classe 0  Classe 1  Classe 1  Classe 0
   ```

3. **Vote final**
   - 100 arbres donnent leur prÃ©diction
   - Si 70 arbres disent "DÃ©faut" et 30 disent "RemboursÃ©" â†’ PrÃ©diction finale : DÃ‰FAUT

**ParamÃ¨tres principaux** :
- `n_estimators` : Nombre d'arbres (ex: 100, 200)
- `max_depth` : Profondeur max de chaque arbre (limite la complexitÃ©)
- `min_samples_split` : Nombre minimum d'Ã©chantillons pour diviser un nÅ“ud
- `class_weight` : Ajuste les poids des classes

**Avantages** :
- Capture les relations non-linÃ©aires
- Robuste aux outliers
- Donne l'importance des variables

**Limites** :
- Plus lent que la rÃ©gression logistique
- Moins interprÃ©table (boÃ®te noire)

---

### **ðŸš€ GRADIENT BOOSTING (XGBoost)**

**Analogie simple** : C'est comme un Ã©lÃ¨ve qui apprend de ses erreurs, encore et encore.

**Comment Ã§a marche ?**

1. **Arbre 1** : Fait des prÃ©dictions (avec plein d'erreurs)
   ```
   Vrai : [0, 1, 0, 1, 0]
   PrÃ©dit : [0, 0, 0, 1, 1]
   Erreur : [0, 1, 0, 0, -1]  â† Ces erreurs
   ```

2. **Arbre 2** : EntraÃ®nÃ© pour **corriger les erreurs de l'Arbre 1**
   ```
   Il apprend Ã  prÃ©dire les erreurs : [0, 1, 0, 0, -1]
   ```

3. **Arbre 3** : EntraÃ®nÃ© pour **corriger les erreurs rÃ©siduelles**
   
4. **PrÃ©diction finale** = Arbre1 + Arbre2 + Arbre3 + ... (avec des poids)

**DiffÃ©rence avec Random Forest** :
- Random Forest : arbres **indÃ©pendants** qui votent
- XGBoost : arbres **sÃ©quentiels** qui se corrigent mutuellement

**ParamÃ¨tres principaux** :
- `n_estimators` : Nombre d'arbres sÃ©quentiels
- `learning_rate` : Vitesse d'apprentissage (petit = plus prudent)
- `max_depth` : Profondeur des arbres
- `subsample` : Proportion de donnÃ©es utilisÃ©es par arbre
- `colsample_bytree` : Proportion de features utilisÃ©es par arbre
- `scale_pos_weight` : Ajuste le poids de la classe positive (dÃ©faut)

**Avantages** :
- Meilleure performance sur donnÃ©es tabulaires
- GÃ¨re nativement les valeurs manquantes
- RÃ©gularisation intÃ©grÃ©e

**Limites** :
- Plus lent Ã  entraÃ®ner
- Plus de paramÃ¨tres Ã  rÃ©gler
- Risque d'overfitting si mal paramÃ©trÃ©

---

## **RÃ‰SUMÃ‰ POUR TA PRÃ‰SENTATION**

### **RÃ©gression Logistique**
"ModÃ¨le linÃ©aire qui calcule un score puis le transforme en probabilitÃ© avec une fonction sigmoÃ¯de. Simple et interprÃ©table, mais limitÃ© aux relations linÃ©aires."

### **Random Forest**
"Ensemble de 100+ arbres de dÃ©cision entraÃ®nÃ©s sur des Ã©chantillons alÃ©atoires. Chaque arbre vote et la majoritÃ© l'emporte. Capture les relations complexes et est robuste aux outliers."

### **XGBoost**
"Construction sÃ©quentielle d'arbres oÃ¹ chaque nouvel arbre corrige les erreurs du prÃ©cÃ©dent. Optimise progressivement les prÃ©dictions. Le plus performant sur donnÃ©es tabulaires."
